{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport gensim.downloader as gd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T17:16:22.831056Z","iopub.execute_input":"2025-05-24T17:16:22.831371Z","iopub.status.idle":"2025-05-24T17:16:22.836946Z","shell.execute_reply.started":"2025-05-24T17:16:22.831346Z","shell.execute_reply":"2025-05-24T17:16:22.836021Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Step 1: Convert text into embeddings","metadata":{}},{"cell_type":"code","source":"print(list(gd.info()['models']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:13.444297Z","iopub.execute_input":"2025-05-24T16:20:13.445228Z","iopub.status.idle":"2025-05-24T16:20:13.501029Z","shell.execute_reply.started":"2025-05-24T16:20:13.445194Z","shell.execute_reply":"2025-05-24T16:20:13.500188Z"}},"outputs":[{"name":"stdout","text":"['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"embedding_model = gd.load('glove-wiki-gigaword-50')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:13.503394Z","iopub.execute_input":"2025-05-24T16:20:13.503711Z","iopub.status.idle":"2025-05-24T16:20:38.969902Z","shell.execute_reply.started":"2025-05-24T16:20:13.503686Z","shell.execute_reply":"2025-05-24T16:20:38.968964Z"}},"outputs":[{"name":"stdout","text":"[=================================-----------------] 66.5% 43.9/66.0MB downloaded\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"text = \"Hello! How are you? Don't you have some work to do?\"\ntext = text.split()\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:38.971013Z","iopub.execute_input":"2025-05-24T16:20:38.971394Z","iopub.status.idle":"2025-05-24T16:20:38.976600Z","shell.execute_reply.started":"2025-05-24T16:20:38.971366Z","shell.execute_reply":"2025-05-24T16:20:38.975846Z"}},"outputs":[{"name":"stdout","text":"['Hello!', 'How', 'are', 'you?', \"Don't\", 'you', 'have', 'some', 'work', 'to', 'do?']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import re\n\n# function to remove punctuations\ndef clean_word(word):\n    return re.sub(r'[^\\w\\s]', \"\", word)\n\ntext = list(map(clean_word, text))\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:38.977592Z","iopub.execute_input":"2025-05-24T16:20:38.978327Z","iopub.status.idle":"2025-05-24T16:20:39.071453Z","shell.execute_reply.started":"2025-05-24T16:20:38.978292Z","shell.execute_reply":"2025-05-24T16:20:39.070433Z"}},"outputs":[{"name":"stdout","text":"['Hello', 'How', 'are', 'you', 'Dont', 'you', 'have', 'some', 'work', 'to', 'do']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"vector = list(map(lambda x : embedding_model[x.lower()], text))\nprint(vector)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.072734Z","iopub.execute_input":"2025-05-24T16:20:39.073005Z","iopub.status.idle":"2025-05-24T16:20:39.096456Z","shell.execute_reply.started":"2025-05-24T16:20:39.072984Z","shell.execute_reply":"2025-05-24T16:20:39.095476Z"}},"outputs":[{"name":"stdout","text":"[array([-0.38497 ,  0.80092 ,  0.064106, -0.28355 , -0.026759, -0.34532 ,\n       -0.64253 , -0.11729 , -0.33257 ,  0.55243 , -0.087813,  0.9035  ,\n        0.47102 ,  0.56657 ,  0.6985  , -0.35229 , -0.86542 ,  0.90573 ,\n        0.03576 , -0.071705, -0.12327 ,  0.54923 ,  0.47005 ,  0.35572 ,\n        1.2611  , -0.67581 , -0.94983 ,  0.68666 ,  0.3871  , -1.3492  ,\n        0.63512 ,  0.46416 , -0.48814 ,  0.83827 , -0.9246  , -0.33722 ,\n        0.53741 , -1.0616  , -0.081403, -0.67111 ,  0.30923 , -0.3923  ,\n       -0.55002 , -0.68827 ,  0.58049 , -0.11626 ,  0.013139, -0.57654 ,\n        0.048833,  0.67204 ], dtype=float32), array([ 6.8938e-01, -1.0644e-01,  1.7083e-01, -3.7583e-01,  7.5170e-01,\n        7.8149e-04, -5.3102e-01, -1.9903e-01, -1.4419e-01,  1.2748e-01,\n       -2.8038e-01,  7.0723e-01, -5.4100e-01,  1.9625e-01,  9.6635e-01,\n        6.0519e-01,  4.0918e-01, -3.1612e-02,  5.3900e-01, -8.7086e-01,\n       -2.0912e-01,  5.6853e-01,  6.5983e-01,  1.4583e-01,  1.0112e+00,\n       -2.0736e+00, -1.1242e+00,  5.9662e-04,  7.0332e-01, -8.2608e-01,\n        3.4445e+00,  3.2984e-01, -3.5324e-01, -1.0335e+00, -1.4753e-01,\n       -1.4874e-01, -4.1246e-01,  3.3489e-01,  1.9841e-01, -2.5478e-01,\n       -4.7193e-01,  6.6701e-02,  3.2777e-01,  6.8781e-01,  3.6428e-01,\n        2.1522e-01,  1.6494e-01,  4.1761e-01, -2.2504e-01,  6.1412e-01],\n      dtype=float32), array([ 0.96193  ,  0.012516 ,  0.21733  , -0.06539  ,  0.26843  ,\n        0.33586  , -0.45112  , -0.60547  , -0.46845  , -0.18412  ,\n        0.060949 ,  0.19597  ,  0.22645  ,  0.032802 ,  0.42488  ,\n        0.49678  ,  0.65346  , -0.0274   ,  0.17809  , -1.1979   ,\n       -0.40634  , -0.22659  ,  1.1495   ,  0.59342  , -0.23759  ,\n       -0.93254  , -0.52502  ,  0.05125  ,  0.032248 , -0.72774  ,\n        4.2466   ,  0.60592  ,  0.33397  , -0.85754  ,  0.4895   ,\n        0.21744  , -0.13451  ,  0.0094912, -0.54173  ,  0.18857  ,\n       -0.64506  ,  0.012695 ,  0.73452  ,  1.0032   ,  0.41874  ,\n        0.16596  , -0.71085  ,  0.14032  , -0.38468  , -0.38712  ],\n      dtype=float32), array([-1.0919e-03,  3.3324e-01,  3.5743e-01, -5.4041e-01,  8.2032e-01,\n       -4.9391e-01, -3.2588e-01,  1.9972e-03, -2.3829e-01,  3.5554e-01,\n       -6.0655e-01,  9.8932e-01, -2.1786e-01,  1.1236e-01,  1.1494e+00,\n        7.3284e-01,  5.1182e-01,  2.9287e-01,  2.8388e-01, -1.3590e+00,\n       -3.7951e-01,  5.0943e-01,  7.0710e-01,  6.2941e-01,  1.0534e+00,\n       -2.1756e+00, -1.3204e+00,  4.0001e-01,  1.5741e+00, -1.6600e+00,\n        3.7721e+00,  8.6949e-01, -8.0439e-01,  1.8390e-01, -3.4332e-01,\n        1.0714e-02,  2.3969e-01,  6.6748e-02,  7.0117e-01, -7.3702e-01,\n        2.0877e-01,  1.1564e-01, -1.5190e-01,  8.5908e-01,  2.2620e-01,\n        1.6519e-01,  3.6309e-01, -4.5697e-01, -4.8969e-02,  1.1316e+00],\n      dtype=float32), array([ 0.7159   ,  0.1473   ,  0.20288  , -0.95569  , -0.46533  ,\n       -1.36     ,  0.02626  ,  0.2658   , -0.065819 ,  0.18268  ,\n       -0.30943  ,  0.69249  , -0.63087  , -0.31461  ,  0.43526  ,\n        0.057878 ,  1.0704   , -0.094101 ,  0.55567  , -0.98373  ,\n       -1.5676   ,  0.30991  ,  0.52536  ,  0.53308  ,  1.0888   ,\n        0.23814  , -1.0548   ,  0.56173  ,  0.50826  , -1.4267   ,\n       -0.060585 ,  1.4045   , -0.7828   ,  0.54784  , -0.51608  ,\n       -0.84208  ,  0.42081  , -0.26491  ,  0.44171  ,  0.65684  ,\n        0.068145 , -0.27055  ,  0.27824  ,  0.61661  , -0.5059   ,\n       -0.40865  ,  0.21496  ,  0.33603  , -0.0082491,  0.91228  ],\n      dtype=float32), array([-1.0919e-03,  3.3324e-01,  3.5743e-01, -5.4041e-01,  8.2032e-01,\n       -4.9391e-01, -3.2588e-01,  1.9972e-03, -2.3829e-01,  3.5554e-01,\n       -6.0655e-01,  9.8932e-01, -2.1786e-01,  1.1236e-01,  1.1494e+00,\n        7.3284e-01,  5.1182e-01,  2.9287e-01,  2.8388e-01, -1.3590e+00,\n       -3.7951e-01,  5.0943e-01,  7.0710e-01,  6.2941e-01,  1.0534e+00,\n       -2.1756e+00, -1.3204e+00,  4.0001e-01,  1.5741e+00, -1.6600e+00,\n        3.7721e+00,  8.6949e-01, -8.0439e-01,  1.8390e-01, -3.4332e-01,\n        1.0714e-02,  2.3969e-01,  6.6748e-02,  7.0117e-01, -7.3702e-01,\n        2.0877e-01,  1.1564e-01, -1.5190e-01,  8.5908e-01,  2.2620e-01,\n        1.6519e-01,  3.6309e-01, -4.5697e-01, -4.8969e-02,  1.1316e+00],\n      dtype=float32), array([ 0.94911  , -0.34968  ,  0.48125  , -0.19306  , -0.0088384,\n        0.28182  , -0.9613   , -0.13581  , -0.43083  , -0.092933 ,\n        0.15689  ,  0.059585 , -0.49635  , -0.17414  ,  0.75661  ,\n        0.4921   ,  0.21773  , -0.22778  , -0.13686  , -0.90589  ,\n       -0.48781  ,  0.19919  ,  0.91447  , -0.16203  , -0.20645  ,\n       -1.7312   , -0.47622  , -0.04854  , -0.14027  , -0.45828  ,\n        4.0326   ,  0.6052   ,  0.10448  , -0.7361   ,  0.2485   ,\n       -0.033461 , -0.13395  ,  0.052782 , -0.27268  ,  0.079825 ,\n       -0.80127  ,  0.30831  ,  0.43567  ,  0.88747  ,  0.29816  ,\n       -0.02465  , -0.95075  ,  0.36233  , -0.72512  , -0.6089   ],\n      dtype=float32), array([ 9.2871e-01, -1.0834e-01,  2.1497e-01, -5.0237e-01,  1.0379e-01,\n        2.2728e-01, -5.4198e-01, -2.9008e-01, -6.4607e-01,  1.2664e-01,\n       -4.1487e-01, -2.9343e-01,  3.6855e-01, -4.1733e-01,  6.9116e-01,\n        6.7341e-02,  1.9715e-01, -3.0465e-02, -2.1723e-01, -1.2238e+00,\n        9.5469e-03,  1.9594e-01,  5.6595e-01, -6.7473e-02,  5.9208e-02,\n       -1.3909e+00, -8.9275e-01, -1.3546e-01,  1.6200e-01, -4.0210e-01,\n        4.1644e+00,  3.7816e-01,  1.5797e-01, -4.8892e-01,  2.3131e-01,\n        2.3258e-01, -2.5314e-01, -1.9977e-01, -1.2258e-01,  1.5620e-01,\n       -3.1995e-01,  3.8314e-01,  4.7266e-01,  8.7700e-01,  3.2223e-01,\n        1.3292e-03, -4.9860e-01,  5.5580e-01, -7.0359e-01, -5.2693e-01],\n      dtype=float32), array([ 5.1359e-01,  1.9695e-01, -5.1944e-01, -8.6218e-01,  1.5494e-02,\n        1.0973e-01, -8.0293e-01, -3.3361e-01, -1.6119e-04,  1.0189e-02,\n        4.6734e-02,  4.6751e-01, -4.7475e-01,  1.1038e-01,  3.9327e-01,\n       -4.3652e-01,  3.9984e-01,  2.7109e-01,  4.2650e-01, -6.0640e-01,\n        8.1145e-01,  4.5630e-01, -1.2726e-01, -2.2474e-01,  6.4071e-01,\n       -1.2767e+00, -7.2231e-01, -6.9590e-01,  2.8045e-02, -2.3072e-01,\n        3.7996e+00, -1.2625e-01, -4.7967e-01, -9.9972e-01, -2.1976e-01,\n        5.0565e-01,  2.5953e-02,  8.0514e-01,  1.9929e-01,  2.8796e-01,\n       -1.5915e-01, -3.0438e-01,  1.6025e-01, -1.8290e-01, -3.8563e-02,\n       -1.7619e-01,  2.7041e-02,  4.6842e-02, -6.2897e-01,  3.5726e-01],\n      dtype=float32), array([ 0.68047 , -0.039263,  0.30186 , -0.17792 ,  0.42962 ,  0.032246,\n       -0.41376 ,  0.13228 , -0.29847 , -0.085253,  0.17118 ,  0.22419 ,\n       -0.10046 , -0.43653 ,  0.33418 ,  0.67846 ,  0.057204, -0.34448 ,\n       -0.42785 , -0.43275 ,  0.55963 ,  0.10032 ,  0.18677 , -0.26854 ,\n        0.037334, -2.0932  ,  0.22171 , -0.39868 ,  0.20912 , -0.55725 ,\n        3.8826  ,  0.47466 , -0.95658 , -0.37788 ,  0.20869 , -0.32752 ,\n        0.12751 ,  0.088359,  0.16351 , -0.21634 , -0.094375,  0.018324,\n        0.21048 , -0.03088 , -0.19722 ,  0.082279, -0.09434 , -0.073297,\n       -0.064699, -0.26044 ], dtype=float32), array([ 2.9605e-01, -1.3841e-01,  4.3774e-02, -3.8744e-01,  1.2262e-01,\n       -6.5180e-01, -2.8240e-01,  9.0312e-02, -5.5186e-01,  3.2060e-01,\n        3.7422e-03,  9.3229e-01, -2.2034e-01, -2.1922e-01,  9.2170e-01,\n        7.5724e-01,  8.4892e-01, -4.2197e-03,  5.3626e-01, -1.2667e+00,\n       -6.1028e-01,  1.6700e-01,  8.2753e-01,  6.5765e-01,  4.8959e-01,\n       -1.9744e+00, -1.1490e+00, -2.1461e-01,  8.0539e-01, -1.4745e+00,\n        3.7490e+00,  1.0141e+00, -1.1293e+00, -5.2661e-01, -1.2029e-01,\n       -2.7931e-01,  6.5092e-02, -4.3639e-02,  6.0426e-01, -2.0892e-01,\n       -4.5739e-01,  1.0441e-02,  4.1458e-01,  6.8900e-01,  1.4468e-01,\n       -3.1973e-02, -4.8073e-02, -1.1279e-04,  1.3854e-01,  9.6954e-01],\n      dtype=float32)]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Transformer Model Implementation","metadata":{}},{"cell_type":"markdown","source":"### Embedding Layer","metadata":{}},{"cell_type":"code","source":"class EmbeddingLayer(nn.Module):\n    def __init__(self):\n        super(EmbeddingLayer, self).__init__()\n        self.model = gd.load('glove-wiki-gigaword-50')\n\n    def forward(self, x):\n        x = list(map(lambda x : self.model[x], x))\n        x = torch.Tensor(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.097621Z","iopub.execute_input":"2025-05-24T16:20:39.098031Z","iopub.status.idle":"2025-05-24T16:20:39.117739Z","shell.execute_reply.started":"2025-05-24T16:20:39.097957Z","shell.execute_reply":"2025-05-24T16:20:39.116709Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Positional Encoding ","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, max_words, d_model):\n        super(PositionalEncoding, self).__init__() # skip the positionalEncoding inside super (after python 3)\n\n        self.matrix = torch.zeros(max_words, d_model)\n        val = torch.Tensor([pos / (10000 ** ((2*i) / d_model)) for pos in torch.arange(max_words) for i in torch.arange(0, d_model, 2)]).reshape(max_words, d_model//2)\n        self.matrix[:, 0::2] = torch.sin(val)\n        self.matrix[:, 1::2] = torch.cos(val)\n\n    def forward(self, x):\n        x += self.matrix\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.118812Z","iopub.execute_input":"2025-05-24T16:20:39.119076Z","iopub.status.idle":"2025-05-24T16:20:39.135912Z","shell.execute_reply.started":"2025-05-24T16:20:39.119052Z","shell.execute_reply":"2025-05-24T16:20:39.134939Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Self attention block","metadata":{}},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, d_model, d_k, d_v):\n        super(SelfAttention, self).__init__() \n        self.d_k = d_k\n        self.d_v = d_v\n        self.d_model = d_model\n        \n        self.w_q = nn.Linear(d_model, d_k)\n        self.w_k = nn.Linear(d_model, d_k)\n        self.w_v = nn.Linear(d_model, d_v)\n\n    def forward(self, q, k, v, mask=None):\n\n        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n\n        score = (q@k.T) / math.sqrt(self.d_k)\n\n        if mask is not None:\n            score = score.masked_fill(mask == 0, -100000)\n\n        x = F.softmax(score, dim=-1) @ v \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.138513Z","iopub.execute_input":"2025-05-24T16:20:39.138833Z","iopub.status.idle":"2025-05-24T16:20:39.154971Z","shell.execute_reply.started":"2025-05-24T16:20:39.138810Z","shell.execute_reply":"2025-05-24T16:20:39.154101Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Multi-Head attention block","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, n_heads, d_model):\n        super(MultiHeadAttention, self).__init__()\n\n        self.n_head = n_heads\n        self.d_model = d_model\n        self.k = self.v = d_model // n_heads\n\n        self.attentions = [SelfAttention(self.d_model, self.k, self.v) for _ in range(n_heads)]\n        self.w_o = nn.Linear(d_model, d_model)\n\n    def forward(self, q, k, v, mask=None):\n        \n        x = [attention(q, k, v, mask) for attention in self.attentions]\n        x = torch.cat(x, dim=1)\n        x = self.w_o(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.155946Z","iopub.execute_input":"2025-05-24T16:20:39.156274Z","iopub.status.idle":"2025-05-24T16:20:39.238702Z","shell.execute_reply.started":"2025-05-24T16:20:39.156244Z","shell.execute_reply":"2025-05-24T16:20:39.237289Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### FeedForward Network","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(FeedForward, self).__init__()\n\n        self.layer1 = nn.Linear(d_model, d_ff)\n        self.layer2 = nn.Linear(d_ff, d_model)\n\n    def forward(self,x):\n        \n        x = self.layer1(x)\n        x = F.relu(x)\n        x = self.layer2(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.240135Z","iopub.execute_input":"2025-05-24T16:20:39.240567Z","iopub.status.idle":"2025-05-24T16:20:39.302230Z","shell.execute_reply.started":"2025-05-24T16:20:39.240513Z","shell.execute_reply":"2025-05-24T16:20:39.301196Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Encoder Block","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, n_heads, d_model, d_ff):\n        super(Encoder, self).__init__()\n\n        self.mha = MultiHeadAttention(n_heads, d_model)\n        self.layernorm1 = nn.LayerNorm(d_model)\n\n        self.feedforward = FeedForward(d_model, d_ff)\n        self.layernorm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # MHA and (Skip + layernorm)\n        x = self.layernorm1(x + self.mha(x, x, x))\n\n        # FeedForward and (Skip + layernorm)\n        x = self.layernorm2(x + self.feedforward(x))\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T17:03:16.095272Z","iopub.execute_input":"2025-05-24T17:03:16.095619Z","iopub.status.idle":"2025-05-24T17:03:16.102046Z","shell.execute_reply.started":"2025-05-24T17:03:16.095596Z","shell.execute_reply":"2025-05-24T17:03:16.101234Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Decoder Block","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, n_heads, d_model, d_ff):\n        super(Decoder, self).__init__()\n\n        self.masked_mha = MultiHeadAttention(n_heads, d_model)\n        self.layernorm1 = nn.LayerNorm(d_model)\n        self.mha = MultiHeadAttention(n_heads, d_model)\n        self.layernorm2 = nn.LayerNorm(d_model)\n        self.ff = FeedForward(d_model, d_ff)\n        self.layernorm3 = nn.LayerNorm(d_model)\n\n    def forward(self, x_encoder, x_prev):\n\n        # masked mha + add & norm\n        mask = np.ones(x_prev.shape)\n        for i in range(x_prev.shape[0]):\n            for j in range(x_prev.shape[1]):\n                mask[i][j] = i <= j\n        \n        x = self.layernorm1(x_prev + self.masked_mha(x_prev, x_prev, x_prev, mask))\n\n        # encoder mha + add & norm\n        x = self.layernorm2(x + self.mha(x, x_encoder, x_encoder))\n\n        # feedforward + add & norm\n        x = self.layernorm3(x + self.ff(x))\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T17:16:53.958298Z","iopub.execute_input":"2025-05-24T17:16:53.958654Z","iopub.status.idle":"2025-05-24T17:16:53.965647Z","shell.execute_reply.started":"2025-05-24T17:16:53.958629Z","shell.execute_reply":"2025-05-24T17:16:53.964685Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Transformer Implementation ","metadata":{}},{"cell_type":"code","source":"class MyTransformer(nn.Module):\n    def __init__(self, \n                 max_words, # total words at a time\n                 d_model, # embedding dimension\n                 n_encoder, # number of encoders\n                 n_decoder, # number of decoders\n                 n_heads,  # for multi_head_attention block\n                 d_ff # for hidden layer in feed forward block\n                ):\n        super(MyTransformer, self).__init__()\n\n        self.n_encoder = n_encoder\n        self.n_decoder = n_decoder\n        \n        # embedding layer\n        self.embedding_layer = EmbeddingLayer()\n\n        # positional encoding\n        self.positional_encoding = PositionalEncoding(max_words, d_model)\n\n        # encoders\n        self.encoders = [Encoder(n_heads, d_model, d_ff) for _ in range(self.n_encoder)]\n\n        # decoders\n        self.decoders = [Decoder(n_heads, d_model, d_ff) for _ in range(self.n_decoder)]\n\n        # linear\n        self.linear = nn.Linear(d_model, d_model)\n\n        # softmax\n        self.sftmax = nn.Softmax(dim=1)\n\n    def forward(self, x_input, x_output):\n        # input embedding\n        # x_input = self.embedding_layer(x_input)\n        x_input = self.positional_encoding(x_input)\n\n        # output embedding\n        # x_output = self.embedding_layer(x_output)\n        x_output = self.positional_encoding(x_output)\n\n        # encoder pass\n        for encoder in self.encoders:\n            x_input = encoder(x_input)\n\n        \n        # decoder pass\n        for decoder in self.decoders:\n            x_output = decoder(x_input, x_output)\n\n        # linear paas\n        x = self.linear(x_output)\n        # x = self.sftmax(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T17:46:53.984195Z","iopub.execute_input":"2025-05-24T17:46:53.984832Z","iopub.status.idle":"2025-05-24T17:46:53.992827Z","shell.execute_reply.started":"2025-05-24T17:46:53.984804Z","shell.execute_reply":"2025-05-24T17:46:53.991818Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"## Rough work","metadata":{}},{"cell_type":"code","source":"max_words = 10\nd_model = 6\n\nmatrix = torch.zeros(max_words, d_model)\nval = torch.Tensor([pos / (10000 ** ((2*i) / d_model)) for pos in torch.arange(max_words) for i in torch.arange(0, d_model, 2)]).reshape(max_words, d_model//2)\nmatrix[:, 0::2] = torch.sin(val)\nmatrix[:, 1::2] = torch.cos(val)\n\nprint(matrix.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.348843Z","iopub.execute_input":"2025-05-24T16:20:39.349153Z","iopub.status.idle":"2025-05-24T16:20:39.479856Z","shell.execute_reply.started":"2025-05-24T16:20:39.349122Z","shell.execute_reply":"2025-05-24T16:20:39.478909Z"}},"outputs":[{"name":"stdout","text":"torch.Size([10, 6])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class SimpleModel(nn.Module):\n    def __init__(self, max_words, d_model):\n        super().__init__()\n\n        self.layer = nn.Linear(d_model, d_model//2)\n\n    def forward(self, x):\n        x = self.layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.480756Z","iopub.execute_input":"2025-05-24T16:20:39.480997Z","iopub.status.idle":"2025-05-24T16:20:39.486501Z","shell.execute_reply.started":"2025-05-24T16:20:39.480978Z","shell.execute_reply":"2025-05-24T16:20:39.485515Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"d_ff = 2 * d_model\nn_encoders = n_decoders = 3\nn_heads = d_model // 2\n\nmodel = MyTransformer(max_words, d_model, n_encoders, n_decoders, n_heads, d_ff)\nx = model(matrix, matrix)\nprint(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T17:47:01.845862Z","iopub.execute_input":"2025-05-24T17:47:01.846161Z","iopub.status.idle":"2025-05-24T17:47:18.369601Z","shell.execute_reply.started":"2025-05-24T17:47:01.846138Z","shell.execute_reply":"2025-05-24T17:47:18.368312Z"}},"outputs":[{"name":"stdout","text":"tensor([[ 0.3100,  0.5627,  1.1991, -0.5743, -1.0250, -0.1144],\n        [ 0.1725,  0.6664,  0.7969, -0.4591, -1.1124,  0.0212],\n        [ 0.0804,  0.4723,  0.4589, -0.6279, -0.9467,  0.1105],\n        [ 0.0753,  0.2308,  0.5175, -1.0255, -0.6617,  0.0654],\n        [ 0.1655,  0.1761,  0.9299, -1.2916, -0.4912, -0.0833],\n        [ 0.3047,  0.3369,  1.3149, -1.0990, -0.6280, -0.2151],\n        [ 0.3426,  0.5006,  1.2809, -0.6603, -0.9456, -0.1604],\n        [ 0.2157,  0.6602,  0.9143, -0.4557, -1.1163, -0.0189],\n        [ 0.0965,  0.5471,  0.5243, -0.5591, -1.0085,  0.0944],\n        [ 0.0754,  0.2810,  0.4591, -0.8967, -0.7446,  0.0856]],\n       grad_fn=<AddmmBackward0>)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"reply = list(map(lambda word : embedding_model.similar_by_vector(word, topn=1), x))\nprint(reply)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T17:47:21.051144Z","iopub.execute_input":"2025-05-24T17:47:21.051456Z","iopub.status.idle":"2025-05-24T17:47:21.086791Z","shell.execute_reply.started":"2025-05-24T17:47:21.051432Z","shell.execute_reply":"2025-05-24T17:47:21.085644Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1928911841.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1928911841.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(word)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_vector\u001b[0;34m(self, vector, topn, restrict_vocab)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \"\"\"\n\u001b[0;32m--> 914\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Key '0.31003668904304504' not present in vocabulary\""],"ename":"KeyError","evalue":"\"Key '0.31003668904304504' not present in vocabulary\"","output_type":"error"}],"execution_count":42},{"cell_type":"code","source":"a = torch.Tensor([[1,2, 3, 4],\n                 [5, 6, 7, 8],\n                 [9, 10, 11, 12],\n                 [13, 14, 15, 16]])\n# soft0 = F.softmax(a, dim=-1)\n# print(soft0)\nb = a * 2\n# print(b)\n\nc = torch.cat((a, b), dim=1)\nprint(c)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T16:20:39.685258Z","iopub.status.idle":"2025-05-24T16:20:39.685602Z","shell.execute_reply.started":"2025-05-24T16:20:39.685430Z","shell.execute_reply":"2025-05-24T16:20:39.685444Z"}},"outputs":[],"execution_count":null}]}